{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5d71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:58:06.662006Z",
     "start_time": "2024-04-02T15:58:05.515153Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import random, pickle \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "# from autosklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import autosklearn\n",
    "from autosklearn.regression import AutoSklearnRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4f3a4-3a62-4b00-a3db-6d0961229eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:58:07.378654Z",
     "start_time": "2024-04-02T15:58:07.354937Z"
    }
   },
   "outputs": [],
   "source": [
    "# you could read as csv or pickle file, specify the path or put the pkl file generated from the data preprocessing notebook here\n",
    "\n",
    "df = pd.read_pickle(\"data_latest_average_non_zero_raw.pkl\")\n",
    "df = df[['input_features', 'pointType_0', 'pointType_1', 'pointType_2',\n",
    "       'pointType_3', 'pointType_4', 'pointType_5', 'pointType_6',\n",
    "       'pointType_7', 'pointType_8', 'pointType_9', 'pointType_10',\n",
    "       'pointType_11', 'pointType_12', 'pointType_13', 'pointType_14',\n",
    "       'pointType_15', 'pointType_16', 'pointType_17', 'pointType_18',\n",
    "       'pointType_19', 'pointType_20', 'pointType_21', 'participant_number',\n",
    "       'left_or_right', 'sample_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8cd08-7847-489c-8126-46027261920e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:58:08.333288Z",
     "start_time": "2024-04-02T15:58:08.314972Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fca48d-ffdd-459c-b45b-56c852c31639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:57:00.320940Z",
     "start_time": "2024-04-02T15:57:00.293029Z"
    }
   },
   "outputs": [],
   "source": [
    "participant_list = df.participant_number.drop_duplicates().to_list()\n",
    "random.Random(420).shuffle(participant_list)\n",
    "print(participant_list)\n",
    "participant_list_train, participant_list_test = train_test_split(participant_list, random_state=115,  train_size=0.85)\n",
    "print(participant_list_train, participant_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_flag = True #enable this flag if you want to do data augmentation\n",
    "\n",
    "df_test = df[df['participant_number'].isin(participant_list_test)] #select test participants\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True) #shuffle and reset_index\n",
    "df_train = df[df['participant_number'].isin(participant_list_train)] #select train participants\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True) #shuffle and reset_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "def roll_images(input_features_inner, pixels, axis_, point=None):\n",
    "    # input_features_inner = df.input_features[591] # for debugging\n",
    "    input_features_inner_np = np.asarray(input_features_inner)\n",
    "    input_features_inner_np_rolled = np.roll(input_features_inner_np.reshape(64,32),pixels,axis=axis_)\n",
    "    \n",
    "    if point is not None:\n",
    "    # Convert point from string to tuple if needed\n",
    "        if isinstance(point, str):\n",
    "            point = tuple(map(float, point[1:-1].split(',')))\n",
    "        \n",
    "        if axis_ == 0:\n",
    "            return [point[0], point[1]+pixels]\n",
    "        elif axis_ == 1:\n",
    "            return [point[0]+pixels, point[1]]\n",
    "        else:\n",
    "            return [point[0]+pixels, point[1]+pixels]\n",
    "    else:\n",
    "        # Flatten the rotated image\n",
    "        # print(\"entered here\")\n",
    "        return input_features_inner_np_rolled.reshape(-1)\n",
    "\n",
    "\n",
    "if data_augmentation_flag == True:\n",
    "\n",
    "    x_or_y_shift_list = [-3,-2,-1,1,2,3]\n",
    "    axis_to_shift_list = [0,1,(0,1)]\n",
    "\n",
    "    df_train_temp_fixed = df_train.copy()\n",
    "\n",
    "    for axis_to_shift in axis_to_shift_list:\n",
    "        for x_or_y_shift in x_or_y_shift_list:\n",
    "\n",
    "            print(axis_to_shift,x_or_y_shift)\n",
    "\n",
    "            df_train_augmented = df_train_temp_fixed.copy()\n",
    "\n",
    "            df_train_augmented['input_features'] = df_train_augmented['input_features'].apply(lambda row: roll_images(row,x_or_y_shift,axis_to_shift))\n",
    "\n",
    "            for i in range(0, 22):\n",
    "                # print(i)\n",
    "                pointType_col = f'pointType_{i}'\n",
    "                # Apply the rotation function to each row in the original point column\n",
    "                df_train_augmented[pointType_col] = df_train_augmented.apply(lambda row: roll_images(row['input_features'], x_or_y_shift,axis_to_shift,point=row[pointType_col]), axis=1)\n",
    "            \n",
    "            df_train = pd.concat([df_train,df_train_augmented])\n",
    "\n",
    "\n",
    "    df_train.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e4476-7c97-400e-ad6b-b04f630fab42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract train and test features and labels\n",
    "\n",
    "arr = df_train[[col for col in df_train.columns if 'pointType' in col]].values\n",
    "arr_np = np.asarray(arr.tolist())\n",
    "y_train = arr_np.reshape(arr_np.shape[0], arr_np.shape[1] * arr_np.shape[2])\n",
    "x_train = np.asarray(df_train.input_features.tolist())\n",
    "\n",
    "\n",
    "arr = df_test[[col for col in df_test.columns if 'pointType' in col]].values\n",
    "arr_np = np.asarray(arr.tolist())\n",
    "y_test = arr_np.reshape(arr_np.shape[0], arr_np.shape[1] * arr_np.shape[2])\n",
    "x_test = np.asarray(df_test.input_features.tolist())\n",
    "\n",
    "#print the size of train and test data for verification\n",
    "x_train.shape, y_train.shape , x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train the model using autosklearn automatic hyperparameter tuning using bayesian optimization for regressor problems, \n",
    "instead of the traditional grid search approaches, the only parameters you need to change are the time limit per each model (to train) \n",
    "and the overall time limit. You could also specify the memory limit (as some models take a lot of memory for big data) and a random seed.\n",
    "'''\n",
    "\n",
    "automl = AutoSklearnRegressor(\n",
    "    time_left_for_this_task=3600*5,\n",
    "    per_run_time_limit=150*2,\n",
    "    memory_limit = 500000,\n",
    "    seed = 14141,\n",
    "    metric = autosklearn.metrics.mean_squared_error,\n",
    "    \n",
    "    # resampling_strategy = 'cv'\n",
    "    resampling_strategy_arguments = {\n",
    "    \"shuffle\": True,        # Whether to shuffle before splitting data\n",
    "    # \"folds\": 3              # Used in 'cv' based resampling strategies\n",
    "    }\n",
    ")\n",
    "automl.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open('data_averaged_time5hours_perRun300_model.pkl', 'wb') as f:\n",
    "    pickle.dump(automl, f)\n",
    "\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9e2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(automl.show_models(), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl.predict(x_test)\n",
    "print(\"Mean absolute error score:\", mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff639f6-1eea-4b2d-8356-18b99c5b7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predictions'] = predictions.tolist()\n",
    "df_test['y_test'] = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this output the test data only for visual verification when used in the original data\n",
    "\n",
    "df_test.to_pickle(\"data_averaged_time5hours_perRun300.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
