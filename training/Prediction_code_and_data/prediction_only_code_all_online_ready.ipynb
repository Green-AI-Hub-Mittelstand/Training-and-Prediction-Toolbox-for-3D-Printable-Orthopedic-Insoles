{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5d71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:58:06.662006Z",
     "start_time": "2024-04-02T15:58:05.515153Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os, csv, json, re\n",
    "\n",
    "import random, pickle \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "# from autosklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import autosklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d823ef",
   "metadata": {},
   "source": [
    "### This Part predicts the coordinates and save it to a predictedpoints.json\n",
    "\n",
    "Inputs are the model file in pkl (you should put the full path here, currently it is in the same folder as the ipynb code file) and it will read the raw_pressure.csv from the path given (currently it is in the same folder with the file).\n",
    "\n",
    "Output is the json file with data points (currently it outputs in the same folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc660047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file should always be called raw_pressure.csv, you can change it in the code, but it is hard coded in the load sample function\n",
    "pressure_folder_path = \".\" # currently it is just the same location as the current file\n",
    "json_output_path = \".\" # currently it outputs to the same folder\n",
    "\n",
    "'''\n",
    "the current model 'data_averaged_time5hours_perRun300_model.pkl' is 1 GB in size because of data augmentation, \n",
    "if you want a less accurate but smaller size model, use the no augmentation model 'data_averaged_time5hours_perRun300_model_no_augmentation.pkl'\n",
    "''' \n",
    "model_file_full_path = 'data_averaged_time5hours_perRun300_model.pkl'  # here should be the full path for loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_xtest(test_path):\n",
    "    sampleDir = os.path.normpath(test_path)\n",
    "    csv_file = os.path.join(sampleDir, \"raw_pressure.csv\")\n",
    "\n",
    "    df_raw = pd.read_csv(csv_file, delimiter=\";\", decimal=\",\", header=None, skiprows=4)\n",
    "    df_raw.columns = [f\"Column{i}\" for i in range(len(df_raw.columns))]\n",
    "    df_raw.drop(columns=\"Column0\", axis=1, inplace=True)\n",
    "    df_raw = df_raw[df_raw.sum(axis=1) != 0]\n",
    "    average_non_zero = df_raw.mean()\n",
    "    pressure_data = average_non_zero.tolist()\n",
    "    \n",
    "    return np.asarray(pressure_data).reshape(1,len(pressure_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = load_sample_xtest(pressure_folder_path)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "with open(model_file_full_path, 'rb') as f:\n",
    "    automl = pickle.load(f)\n",
    "\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9e2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(automl.show_models(), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl.predict(x_test)\n",
    "# print(\"Mean absolute error score:\", mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(df, dtest_path):\n",
    "    sampleDir = os.path.normpath(dtest_path)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    # os.makedirs(sampleDir, exist_ok=True)\n",
    "\n",
    "    # Convert DataFrame to a list of dictionaries (each dictionary represents a point)\n",
    "    points_data = []\n",
    "    for row in df.items():\n",
    "        # print(row[0],row[1])\n",
    "        # print(int(re.findall(r'\\d+', row[0])[0]))\n",
    "        points_data.append({\"points\": row[1].to_list()[0], \"pointType\": int(re.findall(r'\\d+', row[0])[0])})\n",
    "        # print(points_data)\n",
    "    \n",
    "    # print(points_data)\n",
    "    # Write the list of dictionaries to the points.json file\n",
    "    points_file = os.path.join(sampleDir, \"predictedPoints.json\")\n",
    "    # print(points_file)\n",
    "    with open(points_file, 'w') as file:\n",
    "        json.dump(points_data, file, indent=2)  # Indent for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = np.asarray(predictions.tolist())\n",
    "\n",
    "num_columns = 22  # Number of columns (pointType_<num>)\n",
    "arr_3d = y_prediction.reshape(y_prediction.shape[0], num_columns, 2)\n",
    "\n",
    "\n",
    "column_names = [f\"pointType_{i}\" for i in range(num_columns)]\n",
    "data_dict = {col: arr_3d[:, i, :].tolist() for i, col in enumerate(column_names)}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_reversed = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sample(df_reversed,json_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09f193",
   "metadata": {},
   "source": [
    "### This part predicts the insole parameters and print it, save it as 'insole.json' (using the same json output path specified at the beginning), and save all predictions to a csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters_list = ['mfk_1_entlasten', 'mfk_2_entlasten',\n",
    "       'mfk_3_entlasten', 'mfk_4_entlasten', 'mfk_5_entlasten',\n",
    "       'zehe_1_entlasten', 'zehe_2_entlasten', 'zehe_3_entlasten',\n",
    "       'zehe_4_entlasten', 'zehe_5_entlasten', 'pelotten_hoehe',\n",
    "       'pelotten_form', 'laengsgewoelbe_hoehe', 'basis_5_entlasten',\n",
    "       'aussenrand_anheben', 'innenrand_anheben']\n",
    "\n",
    "for feature_num, feature_name in enumerate(parameters_list):\n",
    "\n",
    "\n",
    "    print(\"Current Feature Classifier:\", feature_name)\n",
    "\n",
    "\n",
    "\n",
    "    # load model\n",
    "    with open('data_averaged_time5hours_perRun300_secondary_task_'+ feature_name +'model.pkl', 'rb') as f:\n",
    "        automl = pickle.load(f)\n",
    "\n",
    "    # print(automl.leaderboard())\n",
    "\n",
    "    # pprint(automl.show_models(), indent=4)\n",
    "\n",
    "\n",
    "    predictions = automl.predict(x_test)\n",
    "\n",
    "    print(predictions)\n",
    "\n",
    "    df_reversed[feature_name] = predictions\n",
    "\n",
    "# df_reversed.to_pickle(\"insole_parameters.pkl\")\n",
    "\n",
    "\n",
    "parameters_dict = df_reversed[parameters_list].T.to_dict()[0]\n",
    "\n",
    "# Write the list of dictionaries to the points.json file\n",
    "parameters_file = os.path.join(json_output_path, \"insole.json\")\n",
    "# print(points_file)\n",
    "with open(parameters_file, 'w') as file:\n",
    "    json.dump(parameters_dict, file, indent=2)  # Indent for readability\n",
    "\n",
    "df_reversed.to_csv(\"all_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
