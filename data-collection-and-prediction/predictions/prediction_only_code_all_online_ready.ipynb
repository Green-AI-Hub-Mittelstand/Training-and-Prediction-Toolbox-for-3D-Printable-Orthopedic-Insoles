{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5d71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T15:58:06.662006Z",
     "start_time": "2024-04-02T15:58:05.515153Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os, csv, json, re\n",
    "\n",
    "import random, pickle \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "# from autosklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import autosklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d823ef",
   "metadata": {},
   "source": [
    "### This Part predicts the coordinates and save it to a predictedpoints.json\n",
    "\n",
    "Inputs are the model file in pkl (you should put the full path here, currently it is in the same folder as the ipynb code file) and it will read the raw_pressure.csv from the path given (currently it is in the same folder with the file).\n",
    "\n",
    "Output is the json file with data points (currently it outputs in the same folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc660047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the file should always be called raw_pressure.csv, you can change it in the code, but it is hard coded in the load sample function\n",
    "pressure_folder_path = \".\" # currently it is just the same location as the current file\n",
    "json_output_path = \".\" # currently it outputs to the same folder\n",
    "\n",
    "'''\n",
    "the current model 'data_averaged_time5hours_perRun300_model.pkl' is 1 GB in size because of data augmentation, \n",
    "if you want a less accurate but smaller size model, use the no augmentation model 'data_averaged_time5hours_perRun300_model_no_augmentation.pkl'\n",
    "''' \n",
    "model_file_full_path = './models/data_averaged_time5hours_perRun300_model.pkl'  # here should be the full path for loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_xtest(test_path):\n",
    "    sampleDir = os.path.normpath(test_path)\n",
    "    csv_file = os.path.join(sampleDir, \"raw_pressure.csv\")\n",
    "\n",
    "    df_raw = pd.read_csv(csv_file, delimiter=\";\", decimal=\",\", header=None, skiprows=4)\n",
    "    df_raw.columns = [f\"Column{i}\" for i in range(len(df_raw.columns))]\n",
    "    df_raw.drop(columns=\"Column0\", axis=1, inplace=True)\n",
    "    df_raw = df_raw[df_raw.sum(axis=1) != 0]\n",
    "    average_non_zero = df_raw.mean()\n",
    "    pressure_data = average_non_zero.tolist()\n",
    "    \n",
    "    return np.asarray(pressure_data).reshape(1,len(pressure_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fa4dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = load_sample_xtest(pressure_folder_path)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4609a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank  ensemble_weight                 type      cost    duration\n",
      "model_id                                                                  \n",
      "106          1             0.60  k_nearest_neighbors  0.889486  121.600821\n",
      "82           2             0.16  k_nearest_neighbors  1.018268  275.414093\n",
      "87           3             0.02          extra_trees  1.389995   93.927037\n",
      "65           4             0.06        decision_tree  2.112673   45.150692\n",
      "47           5             0.04        decision_tree  2.136166   32.638068\n",
      "101          6             0.06        decision_tree  2.225635   32.276055\n",
      "94           7             0.04        decision_tree  2.266275   47.853082\n",
      "20           8             0.02  k_nearest_neighbors  2.777961   71.169368\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "with open(model_file_full_path, 'rb') as f:\n",
    "    automl = pickle.load(f)\n",
    "\n",
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd9e2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   20: {   'cost': 2.7779610260679615,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f7ac58b0>,\n",
      "            'ensemble_weight': 0.02,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f77dc340>,\n",
      "            'model_id': 20,\n",
      "            'rank': 1,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f77f9460>,\n",
      "            'sklearn_regressor': KNeighborsRegressor(n_neighbors=10, weights='distance')},\n",
      "    47: {   'cost': 2.136165975998706,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f771d4c0>,\n",
      "            'ensemble_weight': 0.04,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f76a7820>,\n",
      "            'model_id': 47,\n",
      "            'rank': 2,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f76a78e0>,\n",
      "            'sklearn_regressor': DecisionTreeRegressor(max_depth=59, min_samples_leaf=8, min_samples_split=7,\n",
      "                      random_state=14141)},\n",
      "    65: {   'cost': 2.1126730854874314,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f7305910>,\n",
      "            'ensemble_weight': 0.06,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f728eaf0>,\n",
      "            'model_id': 65,\n",
      "            'rank': 3,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f728edf0>,\n",
      "            'sklearn_regressor': DecisionTreeRegressor(max_depth=39, min_samples_leaf=7, min_samples_split=8,\n",
      "                      random_state=14141)},\n",
      "    82: {   'cost': 1.0182677002551233,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f6fb8d60>,\n",
      "            'ensemble_weight': 0.16,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f6e82070>,\n",
      "            'model_id': 82,\n",
      "            'rank': 4,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f6e821c0>,\n",
      "            'sklearn_regressor': KNeighborsRegressor(n_neighbors=3, p=1, weights='distance')},\n",
      "    87: {   'cost': 1.3899948260863082,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f6b860d0>,\n",
      "            'ensemble_weight': 0.02,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f6b7a2b0>,\n",
      "            'model_id': 87,\n",
      "            'rank': 5,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f6b7a5b0>,\n",
      "            'sklearn_regressor': ExtraTreesRegressor(max_features=0.9300909589718883, min_samples_leaf=9,\n",
      "                    min_samples_split=17, n_estimators=512, n_jobs=1,\n",
      "                    random_state=14141, warm_start=True)},\n",
      "    94: {   'cost': 2.266274986479621,\n",
      "            'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f6763d60>,\n",
      "            'ensemble_weight': 0.04,\n",
      "            'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f66eff40>,\n",
      "            'model_id': 94,\n",
      "            'rank': 6,\n",
      "            'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f66af0d0>,\n",
      "            'sklearn_regressor': DecisionTreeRegressor(max_depth=313, min_samples_leaf=8, min_samples_split=4,\n",
      "                      random_state=14141)},\n",
      "    101: {   'cost': 2.2256347707997652,\n",
      "             'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f634f040>,\n",
      "             'ensemble_weight': 0.06,\n",
      "             'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f632b220>,\n",
      "             'model_id': 101,\n",
      "             'rank': 7,\n",
      "             'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f632b2e0>,\n",
      "             'sklearn_regressor': DecisionTreeRegressor(max_depth=1838, min_samples_leaf=11, min_samples_split=5,\n",
      "                      random_state=14141)},\n",
      "    106: {   'cost': 0.8894864145174932,\n",
      "             'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f34f5feb250>,\n",
      "             'ensemble_weight': 0.6,\n",
      "             'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f34f5f5d5b0>,\n",
      "             'model_id': 106,\n",
      "             'rank': 8,\n",
      "             'regressor': <autosklearn.pipeline.components.regression.RegressorChoice object at 0x7f34f5f5d8b0>,\n",
      "             'sklearn_regressor': KNeighborsRegressor(p=1)}}\n"
     ]
    }
   ],
   "source": [
    "pprint(automl.show_models(), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716d6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl.predict(x_test)\n",
    "# print(\"Mean absolute error score:\", mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c46acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(df, dtest_path):\n",
    "    sampleDir = os.path.normpath(dtest_path)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    # os.makedirs(sampleDir, exist_ok=True)\n",
    "\n",
    "    # Convert DataFrame to a list of dictionaries (each dictionary represents a point)\n",
    "    points_data = []\n",
    "    for row in df.items():\n",
    "        # print(row[0],row[1])\n",
    "        # print(int(re.findall(r'\\d+', row[0])[0]))\n",
    "        points_data.append({\"points\": row[1].to_list()[0], \"pointType\": int(re.findall(r'\\d+', row[0])[0])})\n",
    "        # print(points_data)\n",
    "    \n",
    "    # print(points_data)\n",
    "    # Write the list of dictionaries to the points.json file\n",
    "    points_file = os.path.join(sampleDir, \"predictedPoints.json\")\n",
    "    # print(points_file)\n",
    "    with open(points_file, 'w') as file:\n",
    "        json.dump(points_data, file, indent=2)  # Indent for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = np.asarray(predictions.tolist())\n",
    "\n",
    "num_columns = 22  # Number of columns (pointType_<num>)\n",
    "arr_3d = y_prediction.reshape(y_prediction.shape[0], num_columns, 2)\n",
    "\n",
    "\n",
    "column_names = [f\"pointType_{i}\" for i in range(num_columns)]\n",
    "data_dict = {col: arr_3d[:, i, :].tolist() for i, col in enumerate(column_names)}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_reversed = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "889a5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sample(df_reversed,json_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09f193",
   "metadata": {},
   "source": [
    "### This part predicts the insole parameters and print it, save it as 'insole.json' (using the same json output path specified at the beginning), and save all predictions to a csv file for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f90b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Feature Classifier: mfk_1_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: mfk_2_entlasten\n",
      "[3]\n",
      "Current Feature Classifier: mfk_3_entlasten\n",
      "[3]\n",
      "Current Feature Classifier: mfk_4_entlasten\n",
      "[3]\n",
      "Current Feature Classifier: mfk_5_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: zehe_1_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: zehe_2_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: zehe_3_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: zehe_4_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: zehe_5_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: pelotten_hoehe\n",
      "[3]\n",
      "Current Feature Classifier: pelotten_form\n",
      "[1]\n",
      "Current Feature Classifier: laengsgewoelbe_hoehe\n",
      "[4]\n",
      "Current Feature Classifier: basis_5_entlasten\n",
      "[0]\n",
      "Current Feature Classifier: aussenrand_anheben\n",
      "[3]\n",
      "Current Feature Classifier: innenrand_anheben\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters_list = ['mfk_1_entlasten', 'mfk_2_entlasten',\n",
    "       'mfk_3_entlasten', 'mfk_4_entlasten', 'mfk_5_entlasten',\n",
    "       'zehe_1_entlasten', 'zehe_2_entlasten', 'zehe_3_entlasten',\n",
    "       'zehe_4_entlasten', 'zehe_5_entlasten', 'pelotten_hoehe',\n",
    "       'pelotten_form', 'laengsgewoelbe_hoehe', 'basis_5_entlasten',\n",
    "       'aussenrand_anheben', 'innenrand_anheben']\n",
    "\n",
    "for feature_num, feature_name in enumerate(parameters_list):\n",
    "\n",
    "\n",
    "    print(\"Current Feature Classifier:\", feature_name)\n",
    "\n",
    "\n",
    "\n",
    "    # load model\n",
    "    with open(os.path.join(\"./models\", 'data_averaged_time5hours_perRun300_secondary_task_'+ feature_name +'model.pkl'), 'rb') as f:\n",
    "        automl = pickle.load(f)\n",
    "\n",
    "    # print(automl.leaderboard())\n",
    "\n",
    "    # pprint(automl.show_models(), indent=4)\n",
    "\n",
    "\n",
    "    predictions = automl.predict(x_test)\n",
    "\n",
    "    print(predictions)\n",
    "\n",
    "    df_reversed[feature_name] = predictions\n",
    "\n",
    "# df_reversed.to_pickle(\"insole_parameters.pkl\")\n",
    "\n",
    "\n",
    "parameters_dict = df_reversed[parameters_list].T.to_dict()[0]\n",
    "\n",
    "# Write the list of dictionaries to the points.json file\n",
    "parameters_file = os.path.join(json_output_path, \"insole.json\")\n",
    "# print(points_file)\n",
    "with open(parameters_file, 'w') as file:\n",
    "    json.dump(parameters_dict, file, indent=2)  # Indent for readability\n",
    "\n",
    "df_reversed.to_csv(\"all_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96619e9-6d2d-4e88-b77f-d4a4eddfb29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
